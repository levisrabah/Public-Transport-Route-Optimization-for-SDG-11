{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda973e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: []\n",
      "Missing values before cleaning:\n",
      " population_density    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['latitude', 'longitude']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_376720/877731397.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[32m     47\u001b[39m print(\u001b[33m\"Missing values before cleaning:\\n\"\u001b[39m, df.isnull().sum())\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Drop rows with missing latitude or longitude\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m df = df.dropna(subset=[\u001b[33m'latitude'\u001b[39m, \u001b[33m'longitude'\u001b[39m])\n\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Convert latitude and longitude to numeric, handle errors\u001b[39;00m\n\u001b[32m     53\u001b[39m df[\u001b[33m'latitude'\u001b[39m] = pd.to_numeric(df[\u001b[33m'latitude'\u001b[39m], errors=\u001b[33m'coerce'\u001b[39m)\n",
      "\u001b[32m~/PLP/Public Transport Route Optimization for SDG 11/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6673\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6674\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6675\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6676\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6677\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6678\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6679\u001b[39m \n\u001b[32m   6680\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['latitude', 'longitude']"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Load Real Data via Socrata API\n",
    "# Initialize Socrata client\n",
    "# Replace 'your_app_token' with your actual app token from NYC Open Data (optional)\n",
    "# Get app token from: https://data.cityofnewyork.us/profile/edit/developer_settings\n",
    "client = Socrata(\"data.cityofnewyork.us\", \"MTQfj5W0YCakhn6hZuqrYaVML\")  # Use None for no token, or add your token\n",
    "\n",
    "# Fetch NYC Bus Stop Shelters dataset (ID: qafz-7myz)\n",
    "results = client.get(\"qafz-7myz\", limit=10000)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_records(results)\n",
    "\n",
    "# Print column names to identify latitude and longitude fields\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# The dataset may use 'the_geom' (WKT format) or other column names\n",
    "# Check if 'the_geom' exists and extract coordinates\n",
    "if 'the_geom' in df.columns:\n",
    "    # Extract latitude and longitude from 'the_geom' (e.g., POINT (-73.987 40.757))\n",
    "    df['longitude'] = df['the_geom'].apply(lambda x: float(x.split('(')[1].split(' ')[0]) if pd.notnull(x) else np.nan)\n",
    "    df['latitude'] = df['the_geom'].apply(lambda x: float(x.split(' ')[1].split(')')[0]) if pd.notnull(x) else np.nan)\n",
    "else:\n",
    "    # Adjust these names based on actual columns (e.g., 'lat', 'lon')\n",
    "    # Update after checking df.columns\n",
    "    df = df.rename(columns={'lat': 'latitude', 'lon': 'longitude'})  # Example, adjust as needed\n",
    "\n",
    "# Add synthetic population density as a placeholder (replace with real data if available)\n",
    "df['population_density'] = np.random.uniform(low=100, high=10000, size=len(df))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before cleaning:\\n\", df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing latitude or longitude\n",
    "df = df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Convert latitude and longitude to numeric, handle errors\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "# Drop any rows with invalid coordinates\n",
    "df = df.dropna(subset=['latitude', 'longitude'])\n",
    "print(\"Missing values after cleaning:\\n\", df.isnull().sum())\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Step 3: Normalize Features\n",
    "features = ['latitude', 'longitude', 'population_density']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[features])\n",
    "\n",
    "# Step 4: Determine Optimal Number of Clusters (Elbow Method)\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    if k >= 2:  # Silhouette score requires at least 2 clusters\n",
    "        score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K[1:], silhouette_scores[1:], 'bo-')  # Align K[1:] (3-10) with silhouette_scores\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs k')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Train K-means Model (Choose k=5 based on elbow/silhouette analysis)\n",
    "optimal_k = 5\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Step 6: Evaluate Clustering\n",
    "sil_score = silhouette_score(X_scaled, df['cluster'])\n",
    "print(f\"Silhouette Score for k={optimal_k}: {sil_score:.4f}\")\n",
    "\n",
    "# Step 7: Visualize Clusters on Map\n",
    "# Create a map centered on NYC\n",
    "nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "# Add clustered points\n",
    "marker_cluster = MarkerCluster().add_to(nyc_map)\n",
    "for idx, row in df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"Cluster {row['cluster']}, Pop. Density: {row['population_density']:.0f}\",\n",
    "        icon=folium.Icon(color=['red', 'blue', 'green', 'purple', 'orange'][int(row['cluster'])])\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save map\n",
    "nyc_map.save('nyc_bus_stops_map.html')\n",
    "print(\"Map saved as 'nyc_bus_stops_map.html'. Open in a browser to view.\")\n",
    "\n",
    "# Step 8: Visualize Clusters in 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='longitude', y='latitude', hue='cluster', size='population_density',\n",
    "                palette='deep', sizes=(20, 200))\n",
    "plt.title('Clustered Bus Stop Locations by Population Density')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Save Results\n",
    "df.to_csv('clustered_bus_stops.csv', index=False)\n",
    "print(\"Clustered data saved to 'clustered_bus_stops.csv'.\")\n",
    "\n",
    "# Step 10: Summary Statistics by Cluster\n",
    "cluster_summary = df.groupby('cluster').agg({\n",
    "    'latitude': ['mean', 'count'],\n",
    "    'longitude': 'mean',\n",
    "    'population_density': 'mean'\n",
    "}).round(2)\n",
    "print(\"Cluster Summary:\\n\", cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badeb66f-089c-45bc-9196-c86b06632184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
